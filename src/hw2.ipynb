{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E6040 Homework 2 - Programming Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hw2b import load_data, LogisticRegression, HiddenLayer, MLP, myMLP, test_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem b: Bullet 2: Comparison between _tanh_ and _softmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,batch_size=20, \n",
    "         n_hidden=500,n_hiddenLayers=2, activation=T.nnet.softmax,patience=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem b: Bullet 3: Experiment with the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameter tunning \n",
    "learning_rate=[0.01,0.02,0.03,0.04,0.05]\n",
    "n_hidden=[400,500,600,700,800]\n",
    "activation=[T.tanh,T.nnet.softmax,T.nnet.sigmoid ]\n",
    "patience=[8000,9000,10000,11000]\n",
    "\n",
    "best_validation=100\n",
    "\n",
    "for learning_rt in learning_rate:\n",
    "    for n_hidden_nodes in n_hidden:\n",
    "        for activation_fn in activation:\n",
    "            for early_patience in patience:\n",
    "                results = test_mlp(learning_rate=learning_rt, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,batch_size=50, n_hidden=n_hidden_nodes,n_hiddenLayers=3, activation=activation_fn,patience=early_patience, verbose=False)\n",
    "                best_validation_for_model= results[0]\n",
    "                print(('validation error for  %i learning rate, %i n_hidden, %s activation , %i patience '\n",
    "                               'is %f %%') %\n",
    "                      (learning_rt, n_hidden_nodes, activation_fn,early_patience,\n",
    "                       best_validation_for_model* 100.))\n",
    "                if best_validation>best_validation_for_model:\n",
    "                    best_validation = best_validation_for_model\n",
    "                    best_learning_rt = learning_rt\n",
    "                    best_n_hidden = n_hidden_nodes\n",
    "                    best_activation = activation_fn\n",
    "                    best_patience = early_patience \n",
    "\n",
    "n_hidden_layers=[1,3,5,7]\n",
    "test_accuracies=[]\n",
    "for n_hiddenLayers in n_hidden_layers:\n",
    "    results = test_mlp(learning_rate= best_learning_rt, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,batch_size=20, n_hidden=best_n_hidden,n_hiddenLayers=n_hiddenLayers, activation=best_activation_fn,patience=best_patience, verbose=False)\n",
    "    test_accuracies.append(results[1])\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result (reproducing Figure 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document the choice of parameters, and discuss what you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem b: Bullet 4: Experiment with the number of hidden layers, but fix the total number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neurons=840 #use best obtained in the parameter tunning above and same goes for other parameters\n",
    "n_hiddenLayers=[1,2,3,4,5,6]\n",
    "test_accuracies=[]\n",
    "for n_hidden_layers in n_hiddenLayers:\n",
    "    results = test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,batch_size=20, \n",
    "                       n_hidden=n_neurons/n_hidden_layers,n_hiddenLayers=n_hidden_layers, \n",
    "                       activation=T.tanh,patience=10000, verbose=True)\n",
    "    test_accuracies.append((1-results[1])*100)\n",
    "    print(('the validation accuracy is %f %%, for n_hidden_layers %i and the test '\n",
    "           'accuracy is %f %%') % ((1-results[0])*100,n_hidden_layers,(1-results[1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result (reproducing Figure 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document the choice of parameters, and discuss what you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem b: Bullet 5: Experiment with the number of neurons in hidden layers, but fix the number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_neurons=[400,500,600,700,800,900]\n",
    "\n",
    "for n_hidden_neurons in n_neurons:\n",
    "    results=test_mlp(learning_rate=0.01, L1_reg=0.00, L2_reg=0.0001, n_epochs=1000,batch_size=20, \n",
    "                       n_hidden=n_hidden_neurons,n_hiddenLayers=1, activation=T.tanh,patience=10000, verbose=True)\n",
    "    test_accuracies.append((1-results[1])*100)\n",
    "    print(('the validation accuracy is %f %%, for n_neurons %i and the test '\n",
    "           'accuracy is %f %%') % ((1-results[0])*100,n_hidden_layers,(1-results[1])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result (reproducing Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document the choice of parameters, and discuss what you discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem c: Bullet 1: Implement MLP(with dropout) with the parameters specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hw2c import load_data, LogisticRegression, DropoutHiddenLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem c: Bullet 2: Implement MLP(without dropout) with the parameters specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem c: Bullet 2: Compare the two cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
